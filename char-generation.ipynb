{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "## Get the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 1115394 characters\n",
      "=====================================\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_fpath = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt', \n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "text = codecs.open(data_fpath, 'r', encoding='utf8').read()\n",
    "\n",
    "print('Length of the text: {} characters'.format(len(text)))\n",
    "print('=====================================')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "source": [
    "## Extract the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "print ('{} unique characters'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "## Create char2idx / idx2char dictionaries and convert the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n':   0,\n",
      "' ' :   1,\n",
      "'!' :   2,\n",
      "'$' :   3,\n",
      "'&' :   4,\n",
      "\"'\" :   5,\n",
      "',' :   6,\n",
      "'-' :   7,\n",
      "'.' :   8,\n",
      "'3' :   9,\n",
      "':' :  10,\n",
      "';' :  11,\n",
      "'?' :  12,\n",
      "'A' :  13,\n",
      "'B' :  14,\n",
      "'C' :  15,\n",
      "'D' :  16,\n",
      "'E' :  17,\n",
      "'F' :  18,\n",
      "'G' :  19,\n",
      "...\n",
      "=====================================\n",
      "Example of the encoded text: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print('{:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('...')\n",
    "print('=====================================')\n",
    "print('Example of the encoded text: {}'.format(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare TF data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "examples_per_epoch = len(text) // SEQ_LEN\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "dataset = dataset.batch(SEQ_LEN+1, drop_remainder=True)\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "## Take an example from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "=====================================\n",
      "Input data:  \"f of France, but worse than wolves of France,\\nWhose tongue more poisons than the adder's tooth!\\nHow \"\n",
      "Target data: \" of France, but worse than wolves of France,\\nWhose tongue more poisons than the adder's tooth!\\nHow i\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('=====================================')\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()[0]])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()[0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(batch_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.CuDNNGRU(1024,\n",
    "                                 return_sequences=True,\n",
    "                                 recurrent_initializer='glorot_uniform',\n",
    "                                 stateful=True),\n",
    "        tf.keras.layers.Dense(VOCAB_SIZE),\n",
    "    ])\n",
    "\n",
    "model = build_model(batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "![A drawing of the data passing through the model](https://tensorflow.org/tutorials/sequences/images/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(batch_size, sequence_length, vocab_size) : (64, 100, 65)\n",
      "=====================================\n",
      "Input: \n",
      " \"As is a dead man's nose: but I do see't and feel't\\nAs you feel doing thus; and see withal\\nThe instru\"\n",
      "=====================================\n",
      "Next Char Predictions: \n",
      " \"$TdnEeg&y3LJ-XTXyQIM,j'yStlK'KwMtzP&xfVfVuBaL3-MQdKn;3luHZ\\n!DKWSL;htFS\\nsjX&VZvjy?GfhmrTGshltnhqMdvCL\"\n"
     ]
    }
   ],
   "source": [
    "input_example_batch, target_example_batch = list(dataset.take(1))[0]\n",
    "example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(\"(batch_size, sequence_length, vocab_size) : {}\".format(example_batch_predictions.shape))\n",
    "print('=====================================')\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print('=====================================')\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Define loss and prepare for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss=loss)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "## Run the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/174 [============================>.] - ETA: 0s - loss: 2.6585WARNING:tensorflow:From C:\\Users\\LK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "174/174 [==============================] - 21s 123ms/step - loss: 2.6556\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 19s 112ms/step - loss: 1.9375\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 20s 112ms/step - loss: 1.6745\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 20s 116ms/step - loss: 1.5301\n",
      "Epoch 5/10\n",
      "174/174 [==============================] - 21s 119ms/step - loss: 1.4444\n",
      "Epoch 6/10\n",
      "174/174 [==============================] - 21s 119ms/step - loss: 1.3856\n",
      "Epoch 7/10\n",
      "174/174 [==============================] - 23s 131ms/step - loss: 1.3408\n",
      "Epoch 8/10\n",
      "174/174 [==============================] - 21s 123ms/step - loss: 1.3008\n",
      "Epoch 9/10\n",
      "174/174 [==============================] - 22s 124ms/step - loss: 1.2668\n",
      "Epoch 10/10\n",
      "174/174 [==============================] - 22s 129ms/step - loss: 1.2357\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(dataset.repeat(), \n",
    "                    epochs=EPOCHS, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "## Restore the latest checkpoint and rebuild the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "source": [
    "## Define a function for the text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate):\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Beseech you! stand not both his against him.\n",
      "\n",
      "BAMNAND:\n",
      "Give you thus. Lay of never hie how he let\n",
      "Less straight.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, how long you debarr'd; love now it destroy\n",
      "Of Buchol, how loves.\n",
      "\n",
      "JULIET:\n",
      "Another, Pompey; she hath arms, cerhaps breath thy part,\n",
      "Letters for zealou some angled queen\n",
      "Of you, in my fortune llance he would est.\n",
      "Deceited not the news herefore die you give?\n",
      "Then, is true boy? where to your garlen's viewin!\n",
      "To sin me with you!\n",
      "\n",
      "YORK:\n",
      "'byou grant were thing that get you may profandy for you,\n",
      "And what the compositur shall be thyself:\n",
      "So much a milamity.\n",
      "\n",
      "BUSHY:\n",
      "'What! will his mout'st thine eaten, we are but heirro's face.\n",
      "That it didst blead me, thust thou livest thee so of his usfort\n",
      "Unjurged your presence.\n",
      "Thou blasts, power, my lord.\n",
      "\n",
      "KING LEWIS XI:\n",
      "For the drought is twirt ere not in leave?\n",
      "\n",
      "KING RICHARD III:\n",
      "Many fleetings, Doose men, for this good villains.\n",
      "This sighs be wings with warm and weep thee.\n",
      "\n",
      "CAMILLO:\n",
      "Most impatient, sir.\n",
      "\n",
      "ESCALUS:\n",
      "It mis\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO:\", num_generate=1000))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
